{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19c16721-d83d-4ae8-93a1-e0a4b77abf46",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Using HSDS\n",
    "\n",
    "The Highly Scalable Distributed Service (HSDS) is a cloud optimized API to enable access to .h5 files hosted on [AWS](https://registry.opendata.aws). The HSDS software was developed by the [HDF Group](https://www.hdfgroup.org/) and is hosted on Amazon Web Services (AWS) using a combination of EC2 (Elastic Compute) and S3 (Scalable Storage Service). You can read more about the HSDS service [in this slide deck](https://www.slideshare.net/HDFEOS/hdf-cloud-services).\n",
    "\n",
    "\n",
    "#### Setting up HSDS\n",
    "\n",
    "To get started install the h5pyd library:\n",
    "\n",
    "```bash\n",
    "pip install h5pyd\n",
    "```\n",
    "\n",
    "Next, configure h5pyd by running ``hsconfigure`` from the command line, or by\n",
    "creating a configuration file at ``~/.hscfg``:\n",
    "\n",
    "```bash\n",
    "hsconfigure\n",
    "\n",
    "hs_endpoint = https://developer.nrel.gov/api/hsds\n",
    "hs_username =\n",
    "hs_password =\n",
    "hs_api_key = 3K3JQbjZmWctY0xmIfSYvYgtIcM3CN0cb1Y2w9bf\n",
    "```\n",
    "\n",
    "**The example API key here is for demonstration and is rate-limited per IP. To\n",
    "get your own API key, visit https://developer.nrel.gov/signup/**\n",
    "\n",
    "**Please note that our HSDS service is for demonstration purposes only, if you\n",
    "would like to use HSDS for production runs of reV please setup your own\n",
    "service: https://github.com/HDFGroup/hsds and point it to our public HSDS\n",
    "bucket: s3://nrel-pds-hsds**\n",
    "\n",
    "\n",
    "Clone the github - https://github.com/NREL/reV.git and point the TESTDATADIR to tests/data location in your computer.\n",
    "\n",
    "NOTE: In all of these examples, the ``sam_file`` input points to files in\n",
    "the reV test directory [`TESTDATADIR`](https://github.com/NREL/reV/tree/master/tests/data) that may not be copied in your install. You may want to download the relevant SAM system configs from that directory and point the ``sam_file`` variable to the correct filepath on your computer.\n",
    "\n",
    "\n",
    "Also install National Renewable Energy Laboratory's (NREL's) REsource eXtraction tool:\n",
    "\n",
    "```bash\n",
    "pip install NREL-rex\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cc48eb89-92d6-44e2-8c09-b55ab520334b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rex\n",
    "from rex import init_logger\n",
    "import h5pyd\n",
    "import pandas as pd\n",
    "import logging\n",
    "from scipy.spatial import cKDTree\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f5b198f4-562c-46ea-a6a7-cd9f2937379a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_year='2020'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2a008bbc-8302-40f5-82da-8f987ea81d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point this to test data dir where the rev git hub repo is cloned.\n",
    "TESTDATADIR='/Users/lijjumathew/Code/SMU/reV/tests/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b46c8c40-c5df-4760-b59d-715fad1c3a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = '/nrel/nsrdb/v3/nsrdb_{}.h5'.format(data_year)\n",
    "f = h5pyd.File(fp,'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779b1f6f-32f9-4054-b86c-d9c44213ef24",
   "metadata": {},
   "source": [
    "### Nearest Timeseries for given Lat/Lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dba02450-05d6-4bdc-aa8a-e3cfb316acb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Site index for New York City: \t\t 2018268\n",
      "Coordinates of New York City: \t (29.76, 95.36)\n",
      "Coordinates of nearest point: \t [ 51.17 179.42]\n"
     ]
    }
   ],
   "source": [
    "dset_coords = f['coordinates'][...]\n",
    "tree = cKDTree(dset_coords)\n",
    "def nearest_site(tree, lat_coord, lon_coord):\n",
    "    lat_lon = np.array([lat_coord, lon_coord])\n",
    "    dist, pos = tree.query(lat_lon)\n",
    "    return pos\n",
    "\n",
    "NewYorkCity = (29.76,  95.36)\n",
    "NewYorkCity_idx = nearest_site(tree, NewYorkCity[0], NewYorkCity[1] )\n",
    "\n",
    "print(\"Site index for New York City: \\t\\t {}\".format(NewYorkCity_idx))\n",
    "print(\"Coordinates of New York City: \\t {}\".format(NewYorkCity))\n",
    "print(\"Coordinates of nearest point: \\t {}\".format(dset_coords[NewYorkCity_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8b9d10e6-afe8-4004-8bd4-cfbe25a9735a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open .h5 file\n",
    "with h5pyd.File('/nrel/nsrdb/v3/nsrdb_{}.h5'.format(data_year), mode='r') as f:\n",
    "    # Extract time_index and convert to datetime\n",
    "    # NOTE: time_index is saved as byte-strings and must be decoded\n",
    "    time_index = pd.to_datetime(f['time_index'][...].astype(str))\n",
    "    # Initialize DataFrame to store time-series data\n",
    "    time_series = pd.DataFrame(index=time_index)\n",
    "    # Extract variables needed to compute generation from SAM:\n",
    "    for var in ['air_temperature','alpha','aod','asymmetry','cld_opd_dcomp','cld_reff_dcomp','clearsky_dhi','clearsky_dni','clearsky_ghi','cloud_press_acha','cloud_type','dew_point','dhi','dni', 'fill_flag','ghi','ozone','relative_humidity','solar_zenith_angle','ssa','surface_albedo','surface_pressure','total_precipitable_water','wind_direction', 'wind_speed']:\n",
    "    #for var in ['ghi','ozone']:\n",
    "        # Get dataset\n",
    "        ds = f[var]\n",
    "        #print(var)\n",
    "        # attr_list = [key for key in ds.attrs.keys()]\n",
    "        # print(\"Variable:\" + str(var) + \":\" + str(attr_list))\n",
    "        #Extract scale factor\n",
    "        if 'psm_scale_factor' in ds.attrs:\n",
    "            scale_factor = ds.attrs['psm_scale_factor']\n",
    "            # Extract site 100 and add to DataFrame\n",
    "            time_series[var] = ds[:, 1244690] / scale_factor\n",
    "        else:\n",
    "            time_series[var] = ds[:, 1244690]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "875a4aba-18a9-494b-8593-d800d19f8715",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series.index.name = 'timestamp_solar'\n",
    "time_series.to_csv('dallas_solar_energy_{}.csv'.format(data_year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cad8855-9c76-4617-a5aa-fb7c37970030",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
