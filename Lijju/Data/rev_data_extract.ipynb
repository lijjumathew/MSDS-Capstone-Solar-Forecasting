{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19c16721-d83d-4ae8-93a1-e0a4b77abf46",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Using HSDS\n",
    "\n",
    "The Highly Scalable Distributed Service (HSDS) is a cloud optimized API to enable access to .h5 files hosted on [AWS](https://registry.opendata.aws). The HSDS software was developed by the [HDF Group](https://www.hdfgroup.org/) and is hosted on Amazon Web Services (AWS) using a combination of EC2 (Elastic Compute) and S3 (Scalable Storage Service). You can read more about the HSDS service [in this slide deck](https://www.slideshare.net/HDFEOS/hdf-cloud-services).\n",
    "\n",
    "\n",
    "#### Setting up HSDS\n",
    "\n",
    "To get started install the h5pyd library:\n",
    "\n",
    "```bash\n",
    "pip install h5pyd\n",
    "```\n",
    "\n",
    "Next, configure h5pyd by running ``hsconfigure`` from the command line, or by\n",
    "creating a configuration file at ``~/.hscfg``:\n",
    "\n",
    "```bash\n",
    "hsconfigure\n",
    "\n",
    "hs_endpoint = https://developer.nrel.gov/api/hsds\n",
    "hs_username =\n",
    "hs_password =\n",
    "hs_api_key = 3K3JQbjZmWctY0xmIfSYvYgtIcM3CN0cb1Y2w9bf\n",
    "```\n",
    "\n",
    "**The example API key here is for demonstration and is rate-limited per IP. To\n",
    "get your own API key, visit https://developer.nrel.gov/signup/**\n",
    "\n",
    "**Please note that our HSDS service is for demonstration purposes only, if you\n",
    "would like to use HSDS for production runs of reV please setup your own\n",
    "service: https://github.com/HDFGroup/hsds and point it to our public HSDS\n",
    "bucket: s3://nrel-pds-hsds**\n",
    "\n",
    "\n",
    "Clone the github - https://github.com/NREL/reV.git and point the TESTDATADIR to tests/data location in your computer.\n",
    "\n",
    "NOTE: In all of these examples, the ``sam_file`` input points to files in\n",
    "the reV test directory [`TESTDATADIR`](https://github.com/NREL/reV/tree/master/tests/data) that may not be copied in your install. You may want to download the relevant SAM system configs from that directory and point the ``sam_file`` variable to the correct filepath on your computer.\n",
    "\n",
    "\n",
    "Also install National Renewable Energy Laboratory's (NREL's) REsource eXtraction tool:\n",
    "\n",
    "```bash\n",
    "pip install NREL-rex\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc48eb89-92d6-44e2-8c09-b55ab520334b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rex\n",
    "from rex import init_logger\n",
    "import h5pyd\n",
    "import pandas as pd\n",
    "import logging\n",
    "from scipy.spatial import cKDTree\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a008bbc-8302-40f5-82da-8f987ea81d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point this to test data dir where the rev git hub repo is cloned.\n",
    "TESTDATADIR='/Users/lijjumathew/Code/SMU/reV/tests/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b46c8c40-c5df-4760-b59d-715fad1c3a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = '/nrel/nsrdb/v3/nsrdb_2019.h5'\n",
    "f = h5pyd.File(fp,'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdaff63a-6b14-446f-bee3-85dfcdcfaca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['version']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(f.attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54e0bb39-7002-4cf7-bcfa-8e7e9ce55ede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.1.1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.attrs['version'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7844d456-367e-4e63-8e37-f960f9992684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['air_temperature',\n",
       " 'alpha',\n",
       " 'aod',\n",
       " 'asymmetry',\n",
       " 'cld_opd_dcomp',\n",
       " 'cld_reff_dcomp',\n",
       " 'clearsky_dhi',\n",
       " 'clearsky_dni',\n",
       " 'clearsky_ghi',\n",
       " 'cloud_press_acha',\n",
       " 'cloud_type',\n",
       " 'coordinates',\n",
       " 'dew_point',\n",
       " 'dhi',\n",
       " 'dni',\n",
       " 'fill_flag',\n",
       " 'ghi',\n",
       " 'meta',\n",
       " 'ozone',\n",
       " 'relative_humidity',\n",
       " 'solar_zenith_angle',\n",
       " 'ssa',\n",
       " 'surface_albedo',\n",
       " 'surface_pressure',\n",
       " 'time_index',\n",
       " 'total_precipitable_water',\n",
       " 'wind_direction',\n",
       " 'wind_speed']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779b1f6f-32f9-4054-b86c-d9c44213ef24",
   "metadata": {},
   "source": [
    "### Nearest Timeseries for given Lat/Lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dba02450-05d6-4bdc-aa8a-e3cfb316acb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Site index for Dallas City: \t\t 2018268\n",
      "Coordinates of Dallas City: \t (32.77, 96.79)\n",
      "Coordinates of nearest point: \t [ 51.17 179.42]\n"
     ]
    }
   ],
   "source": [
    "dset_coords = f['coordinates'][...]\n",
    "tree = cKDTree(dset_coords)\n",
    "def nearest_site(tree, lat_coord, lon_coord):\n",
    "    lat_lon = np.array([lat_coord, lon_coord])\n",
    "    dist, pos = tree.query(lat_lon)\n",
    "    return pos\n",
    "\n",
    "DallasCity = (32.77,  96.79)\n",
    "DallasCity_idx = nearest_site(tree, DallasCity[0], DallasCity[1] )\n",
    "\n",
    "print(\"Site index for Dallas City: \\t\\t {}\".format(DallasCity_idx))\n",
    "print(\"Coordinates of Dallas City: \\t {}\".format(DallasCity))\n",
    "print(\"Coordinates of nearest point: \\t {}\".format(dset_coords[DallasCity_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b9d10e6-afe8-4004-8bd4-cfbe25a9735a",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 404] Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Open .h5 file\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mh5pyd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/nrel/nsrdb/v3/2019.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# Extract time_index and convert to datetime\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# NOTE: time_index is saved as byte-strings and must be decoded\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     time_index \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(f[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_index\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m))\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Initialize DataFrame to store time-series data\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages/h5pyd/_hl/files.py:246\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, domain, mode, endpoint, username, password, bucket, api_key, use_session, use_cache, use_shared_mem, logger, owner, linked_domain, retries, timeout, **kwds)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rsp\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;66;03m# file must exist\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     http_conn\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 246\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(rsp\u001b[38;5;241m.\u001b[39mstatus_code, rsp\u001b[38;5;241m.\u001b[39mreason)\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rsp\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw-\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;66;03m# Fail if exists\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     http_conn\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 404] Not Found"
     ]
    }
   ],
   "source": [
    "# Open .h5 file\n",
    "with h5pyd.File('/nrel/nsrdb/v3/2019.h5', mode='r') as f:\n",
    "    # Extract time_index and convert to datetime\n",
    "    # NOTE: time_index is saved as byte-strings and must be decoded\n",
    "    time_index = pd.to_datetime(f['time_index'][...].astype(str))\n",
    "    # Initialize DataFrame to store time-series data\n",
    "    time_series = pd.DataFrame(index=time_index)\n",
    "    # Extract variables needed to compute generation from SAM:\n",
    "    for var in ['air_temperature','alpha','aod','asymmetry','cld_opd_dcomp','cld_reff_dcomp','clearsky_dhi','clearsky_dni','clearsky_ghi','cloud_press_acha','cloud_type','dew_point','dhi','dni', 'fill_flag','ghi','ozone','relative_humidity','solar_zenith_angle','ssa','surface_albedo','surface_pressure','total_precipitable_water','wind_direction', 'wind_speed']:\n",
    "    #for var in ['ghi','ozone']:\n",
    "        # Get dataset\n",
    "        ds = f[var]\n",
    "        #print(var)\n",
    "        # attr_list = [key for key in ds.attrs.keys()]\n",
    "        # print(\"Variable:\" + str(var) + \":\" + str(attr_list))\n",
    "        #Extract scale factor\n",
    "        if 'psm_scale_factor' in ds.attrs:\n",
    "            scale_factor = ds.attrs['psm_scale_factor']\n",
    "            # Extract site 100 and add to DataFrame\n",
    "            time_series[var] = ds[:, 1244690] / scale_factor\n",
    "        else:\n",
    "            time_series[var] = ds[:, 1244690]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e62da20-37ae-4a2b-ae5e-fdb61e8c8736",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time_series' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/67/fnznqhqx4vxg2f24_h5hj1dr0000gn/T/ipykernel_91101/3440009402.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtime_series\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'time_series' is not defined"
     ]
    }
   ],
   "source": [
    "time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f676155-36c4-42aa-8e6e-4fb8052fa9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series.index.name = 'timestamp_solar'\n",
    "time_series.to_csv('dallas_solar_energy_2019.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c79128f-d8a5-4678-ab98-2ec4d0173b04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
